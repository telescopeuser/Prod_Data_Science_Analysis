{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Science Analysis for Fraud Detection IT System\n",
    "\n",
    "<img align=\"right\" src='image.png' width=10%>\n",
    "\n",
    "The provided data has the financial transaction data as well as the target variable **isFraud**, which is the actual fraud status of the transaction and **isFlaggedFraud** is the indicator which the current IT system is used to flag the transaction using some threshold.\n",
    "\n",
    "The goal here is to explore the data and build a machine learning model to capture the fraud transaction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Python2 unicode & float-division support:\n",
    "# from __future__ import unicode_literals\n",
    "from __future__ import division"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "%matplotlib inline\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set_style(\"dark\")\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from scipy.stats import skew, boxcox\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Utilities-related functions\n",
    "def now():\n",
    "    tmp = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    return tmp\n",
    "\n",
    "def my_file_read(file):\n",
    "    df = pd.read_csv(file)\n",
    "    print(\"{}: {} has {} observations and {} columns\".format(now(), file, df.shape[0], df.shape[1]))\n",
    "    print(\"{}: Column name checking::: {}\".format(now(), df.columns.tolist()))\n",
    "    return df\n",
    "\n",
    "# Self-defined function to read dataframe and find the missing data on the columns and # of missing\n",
    "def checking_na(df):\n",
    "    try:\n",
    "        if (isinstance(df, pd.DataFrame)):\n",
    "            df_na_bool = pd.concat([df.isnull().any(), df.isnull().sum(), (df.isnull().sum()/df.shape[0])*100],\n",
    "                                   axis=1, keys=['df_bool', 'df_amt', 'missing_ratio_percent'])\n",
    "            df_na_bool = df_na_bool.loc[df_na_bool['df_bool'] == True]\n",
    "            return df_na_bool\n",
    "        else:\n",
    "            print(\"{}: The input is not panda DataFrame\".format(now()))\n",
    "\n",
    "    except (UnboundLocalError, RuntimeError):\n",
    "        print(\"{}: Something is wrong\".format(now()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "raw_data = my_file_read(\"PS_20174392719_1491204439457_log.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quickly look at the dataset sample and other properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "raw_data.head(5)\n",
    "# raw_data.info()\n",
    "# print(checking_na(raw_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "raw_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Exploratory Data Analysis\n",
    "In this section, we will do EDA to understand the data more. From the simulation, there are 5 transaction types as per illustrated below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(raw_data.type.value_counts())\n",
    "\n",
    "f, ax = plt.subplots(1, 1, figsize=(8, 4))\n",
    "raw_data.type.value_counts().plot(kind='bar', title=\"Transaction type\", ax=ax, figsize=(8, 4))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 2 flags:  **isFraud** is the indicator which indicates the actual fraud transactions whereas **isFlaggedFraud** is what the system prevents the transaction due to internal thresholds being triggered.\n",
    "\n",
    "\n",
    "Let's quickly what kinds of transaction are being flagged and are fraud..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# isFraud\n",
    "ax = raw_data.groupby(['type', 'isFraud']).size().plot(kind='bar')\n",
    "ax.set_title(\"# of transaction which are the actual fraud per transaction type\")\n",
    "ax.set_xlabel(\"(Type, isFraud)\")\n",
    "ax.set_ylabel(\"Count of transaction\")\n",
    "for p in ax.patches:\n",
    "    ax.annotate(str(format(int(p.get_height()), ',d')), (p.get_x(), p.get_height()*1.01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# isFlaggedFraud\n",
    "ax = raw_data.groupby(['type', 'isFlaggedFraud']).size().plot(kind='bar')\n",
    "ax.set_title(\"# of transaction which is flagged as fraud per transaction type\")\n",
    "ax.set_xlabel(\"(Type, isFlaggedFraud)\")\n",
    "ax.set_ylabel(\"Count of transaction\")\n",
    "for p in ax.patches:\n",
    "    ax.annotate(str(format(int(p.get_height()), ',d')), (p.get_x(), p.get_height()*1.01))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So it looks the current IT system can flag only 16 transfer transactions as fraud. Let's look at those records and compare with the records which the system cannot catch them.\n",
    "\n",
    "\n",
    "The plot below will also focus only on **transfer** transaction type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 2, figsize=(10, 10))\n",
    "tmp = raw_data.loc[(raw_data.type == 'TRANSFER'), :]\n",
    "\n",
    "a = sns.boxplot(x = 'isFlaggedFraud', y = 'amount', data = tmp, ax=axs[0][0])\n",
    "axs[0][0].set_yscale('log')\n",
    "b = sns.boxplot(x = 'isFlaggedFraud', y = 'oldbalanceDest', data = tmp, ax=axs[0][1])\n",
    "axs[0][1].set(ylim=(0, 0.5e8))\n",
    "c = sns.boxplot(x = 'isFlaggedFraud', y = 'oldbalanceOrg', data=tmp, ax=axs[1][0])\n",
    "axs[1][0].set(ylim=(0, 3e7))\n",
    "d = sns.regplot(x = 'oldbalanceOrg', y = 'amount', data=tmp.loc[(tmp.isFlaggedFraud ==1), :], ax=axs[1][1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Modeling & Data Pre-processing\n",
    "In this section, we will focus only **Transfer** and **Cash Out** transaction types, as they have been identified as fraud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from statsmodels.tools import categorical\n",
    "\n",
    "# 1. Keep only interested transaction type ('TRANSFER', 'CASH_OUT')\n",
    "# 2. Drop some columns\n",
    "# 3. Convert categorical variables to numeric variable\n",
    "tmp = raw_data.loc[(raw_data['type'].isin(['TRANSFER', 'CASH_OUT'])),:]\n",
    "tmp.drop(['step', 'nameOrig', 'nameDest', 'isFlaggedFraud'], axis=1, inplace=True)\n",
    "tmp = tmp.reset_index(drop=True)\n",
    "a = np.array(tmp['type'])\n",
    "b = categorical(a, drop=True)\n",
    "tmp['type_num'] = b.argmax(1)\n",
    "\n",
    "tmp.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the correlation of the selected datapoint from above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def correlation_plot(df):\n",
    "    fig = plt.figure(figsize=(10, 10))\n",
    "    ax1 = fig.add_subplot(111)\n",
    "    cmap = cm.get_cmap('jet', 30)\n",
    "    cax = ax1.imshow(df.corr(), interpolation = \"nearest\", cmap = cmap)\n",
    "    ax1.grid(True)\n",
    "    plt.title(\"Correlation Heatmap\")\n",
    "    labels = df.columns.tolist()\n",
    "    ax1.set_xticklabels(labels, fontsize=13, rotation=45)\n",
    "    ax1.set_yticklabels(labels, fontsize=13)\n",
    "    fig.colorbar(cax)\n",
    "    plt.show()\n",
    "    \n",
    "# correlation_plot(tmp)\n",
    "\n",
    "# Alternatively, we can use quick seaborn\n",
    "# plot the heatmap\n",
    "sns.heatmap(tmp.corr())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quickly get the count and the target variable count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print('Total unber of transactions: %d' len(tmp))\n",
    "print('Total unber of transactions: %d', len(tmp))\n",
    "\n",
    "# print(pd.value_counts(tmp['isFraud']) / len(tmp))\n",
    "# 0.99703545577566344897089202352432\n",
    "\n",
    "ax = tmp.type.value_counts().plot(kind='bar', title=\"Transaction type\", figsize=(8, 4))\n",
    "for p in ax.patches:\n",
    "    ax.annotate(str(format(int(p.get_height()), ',d')), (p.get_x(), p.get_height()*1.01))\n",
    "\n",
    "plt.show()\n",
    "\n",
    "ax = pd.value_counts(tmp['isFraud'], sort = True).sort_index().plot(kind='bar', title=\"Fraud transaction count\", figsize=(8, 4))\n",
    "for p in ax.patches:\n",
    "    ax.annotate(str(format(int(p.get_height()), ',d')), (p.get_x(), p.get_height()))\n",
    "    \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the dataset, the numeric variables are quite skew, in this case. Let's scale it with 2 methods and compare them on the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tmp['amount_boxcox'] = preprocessing.scale(boxcox(tmp['amount']+1)[0])\n",
    "\n",
    "figure = plt.figure(figsize=(16, 5))\n",
    "figure.add_subplot(131) \n",
    "plt.hist(tmp['amount'] ,facecolor='blue',alpha=0.75) \n",
    "plt.xlabel(\"Transaction amount\") \n",
    "plt.title(\"Transaction amount \") \n",
    "plt.text(10,100000,\"Skewness: {0:.2f}\".format(skew(tmp['amount'])))\n",
    "\n",
    "figure.add_subplot(132)\n",
    "plt.hist(np.sqrt(tmp['amount']), facecolor = 'red', alpha=0.5)\n",
    "plt.xlabel(\"Square root of amount\")\n",
    "plt.title(\"Using SQRT on amount\")\n",
    "plt.text(10, 100000, \"Skewness: {0:.2f}\".format(skew(np.sqrt(tmp['amount']))))\n",
    "\n",
    "figure.add_subplot(133)\n",
    "plt.hist(tmp['amount_boxcox'], facecolor = 'red', alpha=0.5)\n",
    "plt.xlabel(\"Box cox of amount\")\n",
    "plt.title(\"Using Box cox on amount\")\n",
    "plt.text(10, 100000, \"Skewness: {0:.2f}\".format(skew(tmp['amount_boxcox'])))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tmp['oldbalanceOrg_boxcox'] = preprocessing.scale(boxcox(tmp['oldbalanceOrg']+1)[0])\n",
    "\n",
    "figure = plt.figure(figsize=(16, 5))\n",
    "figure.add_subplot(131) \n",
    "plt.hist(tmp['oldbalanceOrg'] ,facecolor='blue',alpha=0.75) \n",
    "plt.xlabel(\"old balance originated\") \n",
    "plt.title(\"Old balance org\") \n",
    "plt.text(2,100000,\"Skewness: {0:.2f}\".format(skew(tmp['oldbalanceOrg'])))\n",
    "\n",
    "\n",
    "figure.add_subplot(132)\n",
    "plt.hist(np.sqrt(tmp['oldbalanceOrg']), facecolor = 'red', alpha=0.5)\n",
    "plt.xlabel(\"Square root of oldBal\")\n",
    "plt.title(\"SQRT on oldbalanceOrg\")\n",
    "plt.text(2, 100000, \"Skewness: {0:.2f}\".format(skew(np.sqrt(tmp['oldbalanceOrg']))))\n",
    "\n",
    "figure.add_subplot(133)\n",
    "plt.hist(tmp['oldbalanceOrg_boxcox'], facecolor = 'red', alpha=0.5)\n",
    "plt.xlabel(\"Box cox of oldBal\")\n",
    "plt.title(\"Box cox on oldbalanceOrg\")\n",
    "plt.text(2, 100000, \"Skewness: {0:.2f}\".format(skew(tmp['oldbalanceOrg_boxcox'])))\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tmp['newbalanceOrg_boxcox'] = preprocessing.scale(boxcox(tmp['newbalanceOrig']+1)[0])\n",
    "\n",
    "figure = plt.figure(figsize=(16, 5))\n",
    "figure.add_subplot(131) \n",
    "plt.hist(tmp['newbalanceOrig'] ,facecolor='blue',alpha=0.75) \n",
    "plt.xlabel(\"New balance originated\") \n",
    "plt.title(\"New balance org\") \n",
    "plt.text(2,100000,\"Skewness: {0:.2f}\".format(skew(tmp['newbalanceOrig'])))\n",
    "\n",
    "\n",
    "figure.add_subplot(132)\n",
    "plt.hist(np.sqrt(tmp['newbalanceOrig']), facecolor = 'red', alpha=0.5)\n",
    "plt.xlabel(\"Square root of newBal\")\n",
    "plt.title(\"SQRT on newbalanceOrig\")\n",
    "plt.text(2, 100000, \"Skewness: {0:.2f}\".format(skew(np.sqrt(tmp['newbalanceOrig']))))\n",
    "\n",
    "figure.add_subplot(133)\n",
    "plt.hist(tmp['newbalanceOrg_boxcox'], facecolor = 'red', alpha=0.5)\n",
    "plt.xlabel(\"Box cox of newBal\")\n",
    "plt.title(\"Box cox on newbalanceOrig\")\n",
    "plt.text(2, 100000, \"Skewness: {0:.2f}\".format(skew(tmp['newbalanceOrg_boxcox'])))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tmp['oldbalanceDest_boxcox'] = preprocessing.scale(boxcox(tmp['oldbalanceDest']+1)[0])\n",
    "\n",
    "figure = plt.figure(figsize=(16, 5))\n",
    "figure.add_subplot(131) \n",
    "plt.hist(tmp['oldbalanceDest'] ,facecolor='blue',alpha=0.75) \n",
    "plt.xlabel(\"Old balance desinated\") \n",
    "plt.title(\"Old balance dest\") \n",
    "plt.text(2,100000,\"Skewness: {0:.2f}\".format(skew(tmp['oldbalanceDest'])))\n",
    "\n",
    "\n",
    "figure.add_subplot(132)\n",
    "plt.hist(np.sqrt(tmp['oldbalanceDest']), facecolor = 'red', alpha=0.5)\n",
    "plt.xlabel(\"Square root of oldBalDest\")\n",
    "plt.title(\"SQRT on oldbalanceDest\")\n",
    "plt.text(2, 100000, \"Skewness: {0:.2f}\".format(skew(np.sqrt(tmp['oldbalanceDest']))))\n",
    "\n",
    "figure.add_subplot(133)\n",
    "plt.hist(tmp['oldbalanceDest_boxcox'], facecolor = 'red', alpha=0.5)\n",
    "plt.xlabel(\"Box cox of oldbalanceDest\")\n",
    "plt.title(\"Box cox on oldbalanceDest\")\n",
    "plt.text(2, 100000, \"Skewness: {0:.2f}\".format(skew(tmp['oldbalanceDest_boxcox'])))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tmp['newbalanceDest_boxcox'] = preprocessing.scale(boxcox(tmp['newbalanceDest']+1)[0])\n",
    "\n",
    "figure = plt.figure(figsize=(16, 5))\n",
    "figure.add_subplot(131) \n",
    "plt.hist(tmp['newbalanceDest'] ,facecolor='blue',alpha=0.75) \n",
    "plt.xlabel(\"newbalanceDest\") \n",
    "plt.title(\"newbalanceDest\") \n",
    "plt.text(2,100000,\"Skewness: {0:.2f}\".format(skew(tmp['newbalanceDest'])))\n",
    "\n",
    "\n",
    "figure.add_subplot(132)\n",
    "plt.hist(np.sqrt(tmp['newbalanceDest']), facecolor = 'red', alpha=0.5)\n",
    "plt.xlabel(\"Square root of newbalanceDest\")\n",
    "plt.title(\"SQRT on newbalanceDest\")\n",
    "plt.text(2, 100000, \"Skewness: {0:.2f}\".format(skew(np.sqrt(tmp['newbalanceDest']))))\n",
    "\n",
    "figure.add_subplot(133)\n",
    "plt.hist(tmp['newbalanceDest_boxcox'], facecolor = 'red', alpha=0.5)\n",
    "plt.xlabel(\"Box cox of newbalanceDest\")\n",
    "plt.title(\"Box cox on newbalanceDest\")\n",
    "plt.text(2, 100000, \"Skewness: {0:.2f}\".format(skew(tmp['newbalanceDest_boxcox'])))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"The fraud transaction of the filtered dataset: {0:.4f}%\".format((len(tmp[tmp.isFraud == 1])/len(tmp)) * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We filtered unrelated transaction type out and keep only relevant. There're 0.3% actual fraud. This is very imbalance data.\n",
    "\n",
    "\n",
    "We then use traditional under-sampling method (there are several other ways; under and over sampling, SMOTE, etc). Sample the dataset by creating a 50-50 ratio of randomly selecting 'x' amount of sample from majority class, with 'x' being the total number of records with the minority class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tmp.drop(['oldbalanceOrg', 'newbalanceOrig', 'oldbalanceDest', 'newbalanceDest', 'amount', 'type'], axis=1, inplace=True)\n",
    "\n",
    "X = tmp.ix[:, tmp.columns != 'isFraud']\n",
    "y = tmp.ix[:, tmp.columns == 'isFraud']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Number of data points in the minority class\n",
    "number_records_fraud = len(tmp[tmp.isFraud == 1])\n",
    "fraud_indices = tmp[tmp.isFraud == 1].index.values\n",
    "\n",
    "# Picking the indices of the normal classes\n",
    "normal_indices = tmp[tmp.isFraud == 0].index\n",
    "\n",
    "# Out of the indices we picked, randomly select \"x\" number (x - same as total fraud)\n",
    "random_normal_indices = np.random.choice(normal_indices, number_records_fraud, replace = False)\n",
    "random_normal_indices = np.array(random_normal_indices)\n",
    "\n",
    "# Appending the 2 indices\n",
    "under_sample_indices = np.concatenate([fraud_indices,random_normal_indices])\n",
    "under_sample_data = tmp.iloc[under_sample_indices, :]\n",
    "\n",
    "X_undersample = under_sample_data.ix[:, under_sample_data.columns != 'isFraud']\n",
    "y_undersample = under_sample_data.ix[:, under_sample_data.columns == 'isFraud']\n",
    "\n",
    "# Showing ratio\n",
    "print(\"Percentage of normal transactions: \", len(under_sample_data[under_sample_data.isFraud == 0])/len(under_sample_data))\n",
    "print(\"Percentage of fraud transactions: \", len(under_sample_data[under_sample_data.isFraud == 1])/len(under_sample_data))\n",
    "print(\"Total number of transactions in resampled data: \", len(under_sample_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Machine Learning: Simple Logistic Regression Classifier\n",
    "\n",
    "From the model evaluation (or confusion matrix), we know that\n",
    "\n",
    " 1. Accuracy = (TP + TN) / Total\n",
    "\n",
    " 2. Recall = TP / (TP + FN)\n",
    "\n",
    " 3. Presicion = TP / (TP + FP)\n",
    "  \n",
    " 4. F Score = 2 ( Recall \\* Presicion ) / ( Recall + Presicion )\n",
    "\n",
    "\n",
    "We are interested in the recall score to capture the most fraudulent transactions. As we know, due to the imbalance of the data, many observations could be predicted as False Negatives (missed fraudulent transactions), being, that we predict a normal transaction, but it is in fact a fraudulent one. Recall captures this.\n",
    "\n",
    "\n",
    "Obviously, trying to increase recall, tends to come with a decrease of precision. However, in our case, if we predict that a transaction is fraudulent and turns out not to be. These are False Positive (false alarms).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "\n",
    "# Whole dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)\n",
    "\n",
    "# print(\"Number transactions train dataset: \", format(len(X_train),',d'))\n",
    "# print(\"Number transactions test dataset: \", format(len(X_test), ',d'))\n",
    "# print(\"Total number of transactions: \", format(len(X_train)+len(X_test), ',d'))\n",
    "\n",
    "# Undersampled dataset\n",
    "X_train_undersample, X_test_undersample, y_train_undersample, y_test_undersample = train_test_split(X_undersample\n",
    "                                                                                                   ,y_undersample\n",
    "                                                                                                   ,test_size = 0.3\n",
    "                                                                                                   ,random_state = 0)\n",
    "print(\"\")\n",
    "print(\"Number transactions train dataset: \", format(len(X_train_undersample),',d'))\n",
    "print(\"Number transactions test dataset: \", format(len(X_test_undersample),',d'))\n",
    "print(\"Total number of transactions: \", format(len(X_train_undersample)+len(X_test_undersample),',d'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# inputs (predictors)\n",
    "X_train_undersample.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# target (to be predicted)\n",
    "y_train_undersample.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import svm  # Support Vector Machine\n",
    "from sklearn.cross_validation import KFold, cross_val_score\n",
    "from sklearn.metrics import confusion_matrix, precision_score, precision_recall_curve,auc,roc_auc_score,roc_curve,recall_score,classification_report \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def printing_Kfold_scores(x_train_data, y_train_data, kfoldnum, c_array):\n",
    "    # define K-Fold\n",
    "    fold = KFold(len(y_train_data), kfoldnum,shuffle=False) \n",
    "\n",
    "    results_table = pd.DataFrame(index = range(len(c_array),4), columns = ['C_parameter','Mean recall score', 'Mean precision score', 'F Score'])\n",
    "    results_table['C_parameter'] = c_array\n",
    "\n",
    "    # the k-fold will give 2 lists: train_indices = indices[0], test_indices = indices[1]\n",
    "    j = 0\n",
    "    for c_param in c_array:\n",
    "        print('-------------------------------------------')\n",
    "        print('C parameter: ', c_param)\n",
    "        print('-------------------------------------------')\n",
    "        print('')\n",
    "\n",
    "        recall_accs = []\n",
    "        precision_accs = []\n",
    "        for iteration, indices in enumerate(fold,start=1):\n",
    "\n",
    "            # Call the logistic regression model with a certain C parameter\n",
    "            lr = LogisticRegression(C = c_param, penalty = 'l1')\n",
    "\n",
    "            # Use the training data to fit the model. In this case, we use the portion of the fold to train the model\n",
    "            # with indices[0]. We then predict on the portion assigned as the 'test cross validation' with indices[1]\n",
    "            lr.fit(x_train_data.iloc[indices[0],:],y_train_data.iloc[indices[0],:].values.ravel())\n",
    "\n",
    "            # Predict values using the test indices in the training data\n",
    "            y_pred_undersample = lr.predict(x_train_data.iloc[indices[1],:].values)\n",
    "\n",
    "            # Calculate the recall score and append it to a list for recall scores representing the current c_parameter\n",
    "            recall_acc = recall_score(y_train_data.iloc[indices[1],:].values,y_pred_undersample)\n",
    "            recall_accs.append(recall_acc)\n",
    "            \n",
    "            precision_acc = precision_score(y_train_data.iloc[indices[1], :].values, y_pred_undersample)\n",
    "            precision_accs.append(precision_acc)\n",
    "            print(\"K-Fold Iteration {}: recall = {:.4f}, precision = {:.4f}\".format(iteration, recall_acc, precision_acc))\n",
    "\n",
    "        # The mean value of those recall scores is the metric we want to save and get hold of.\n",
    "        results_table.ix[j,'Mean recall score'] = np.mean(recall_accs)\n",
    "        results_table.ix[j, 'Mean precision score'] = np.mean(precision_accs)\n",
    "        results_table.ix[j, 'F Score'] = 2 * np.mean(recall_accs) * np.mean(precision_accs)/(np.mean(recall_accs) + np.mean(precision_accs))\n",
    "        j += 1\n",
    "        print('')\n",
    "        print('Mean recall score {:.4f}'.format(np.mean(recall_accs)))\n",
    "        print('Mean precision score {:.4f}'.format(np.mean(precision_accs)))\n",
    "        print('F Score {:.4f}'.format( 2 * np.mean(recall_accs) * np.mean(precision_accs)/(np.mean(recall_accs) + np.mean(precision_accs)) ))\n",
    "        print('')\n",
    "\n",
    "    best_c = results_table.loc[results_table['F Score'].idxmax()]['C_parameter']\n",
    "    \n",
    "    # Finally, we can check which C parameter is the best amongst the chosen.\n",
    "    print('*********************************************************************************')\n",
    "    print('Best model to choose from cross validation is with C parameter = ', best_c)\n",
    "    print('*********************************************************************************')\n",
    "    \n",
    "    return best_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "c_param_range = [0.001, 0.01, 0.1, 1, 10, 100, 1000]\n",
    "k_fold = 5\n",
    "best_c = printing_Kfold_scores(X_train_undersample,y_train_undersample, k_fold, c_param_range)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Evaluate Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=0)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        #print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        1#print('Confusion matrix, without normalization')\n",
    "\n",
    "    #print(cm)\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "lr = LogisticRegression(C = best_c, penalty = 'l1')\n",
    "lr.fit(X_train_undersample,y_train_undersample.values.ravel())\n",
    "y_pred_undersample = lr.predict(X_test_undersample.values)\n",
    "\n",
    "# Compute confusion matrix\n",
    "cnf_matrix = confusion_matrix(y_test_undersample,y_pred_undersample)\n",
    "\n",
    "print(\"Recall    in the testing dataset: {0:.4f}\".format(cnf_matrix[1,1]/(cnf_matrix[1,0]+cnf_matrix[1,1])))\n",
    "print(\"Precision in the testing dataset: {0:.4f}\".format(cnf_matrix[1,1]/(cnf_matrix[0,1]+cnf_matrix[1,1])))\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "class_names = [0,1]\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix\n",
    "                      , classes=class_names\n",
    "                      , title='Confusion matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# ROC CURVE\n",
    "lr = LogisticRegression(C = best_c, penalty = 'l1')\n",
    "y_pred_undersample_score = lr.fit(X_train_undersample,y_train_undersample.values.ravel()).decision_function(X_test_undersample.values)\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_test_undersample.values.ravel(),y_pred_undersample_score)\n",
    "roc_auc = auc(fpr,tpr)\n",
    "\n",
    "# Plot ROC\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.plot(fpr, tpr, 'b',label='AUC = %0.2f'% roc_auc)\n",
    "plt.legend(loc='lower right')\n",
    "plt.plot([0,1],[0,1],'r--')\n",
    "plt.xlim([-0.1,1.0])\n",
    "plt.ylim([-0.1,1.01])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End of Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Support Vector Machine Model\n",
    "#lr = svm.SVC(kernel = 'poly')\n",
    "#lr = svm.SVC(kernel = 'rbf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "lr = LogisticRegression(C = best_c, penalty = 'l1')\n",
    "lr.fit(X_train_undersample,y_train_undersample.values.ravel())\n",
    "y_pred = lr.predict(X_test.values)\n",
    "\n",
    "# Compute confusion matrix\n",
    "cnf_matrix = confusion_matrix(y_test,y_pred)\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "print(\"Recall    in the testing dataset: {0:.4f}\".format(cnf_matrix[1,1]/(cnf_matrix[1,0]+cnf_matrix[1,1])))\n",
    "print(\"Precision in the testing dataset: {0:.4f}\".format(cnf_matrix[1,1]/(cnf_matrix[0,1]+cnf_matrix[1,1])))\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "class_names = [0,1]\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix\n",
    "                      , classes=class_names\n",
    "                      , title='Confusion matrix')\n",
    "plt.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
